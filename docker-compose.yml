version: '3.8'

services:
  # Frontend React Service
  frontend:
    build:
      context: ./frontend-react
      target: production
    ports:
      - "5173:5173"
    volumes:
      - ./frontend-react:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:5000
      - NODE_ENV=development
    command: npm run dev -- --host 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5173"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - backend

  # Flask Backend Service
  backend:
    build:
      context: ./backend-flask
      target: production
    ports:
      - "5000:5000"
    volumes:
      - ./backend-flask:/app
      - /app/.venv
      - ./backend-flask/app/templates:/app/app/templates
    environment:
      - FLASK_APP=app
      - FLASK_ENV=development
      - FLASK_DEBUG=1
      - CORS_ALLOWED_ORIGINS=http://localhost:5173,http://localhost:80,http://localhost:5000
    command: poetry run flask run --host=0.0.0.0 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - embedding
      - llm

  # OpenAI Components Services
  embedding:
    build: 
      context: ./opea-comps
      dockerfile: Dockerfile
    environment:
      - SERVICE_TYPE=embedding
      - SERVICE_PORT=6000
    ports:
      - "6000:6000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  llm:
    build: 
      context: ./opea-comps
      dockerfile: Dockerfile
    environment:
      - SERVICE_TYPE=llm
      - SERVICE_PORT=9000
    ports:
      - "9000:9000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  mega:
    build: 
      context: ./opea-comps
      dockerfile: Dockerfile
    environment:
      - SERVICE_TYPE=mega
      - SERVICE_PORT=8000
      - EMBEDDING_SERVICE_HOST_IP=embedding
      - EMBEDDING_SERVICE_PORT=6000
      - LLM_SERVICE_HOST_IP=llm
      - LLM_SERVICE_PORT=9000
    ports:
      - "8000:8000"
    depends_on:
      - embedding
      - llm

  vocab-importer:
    build: 
      context: ./vocab-importer
      target: production
    ports:
      - "5001:5001"
    environment:
      - BACKEND_URL=http://backend:5000
      - FRONTEND_URL=http://frontend:5173
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      backend:
        condition: service_healthy
    labels:
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=5001"
      - "prometheus.io/path=/metrics"

  opea-comps:
    build:
      context: ./opea-comps
      dockerfile: Dockerfile
    ports:
      - "8000-8002:8000-8002"
    env_file:
      - ./opea-comps/.env
    volumes:
      - ./opea-comps/comps:/app/comps
    networks:
      - ai-network

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    depends_on:
      - vocab-importer

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

networks:
  ai-network:
    driver: bridge 